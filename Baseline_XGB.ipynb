{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost baselines on housing and french motor datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "# Feature selection strategies\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Scale feature scores\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# SKLearn estimators list\n",
    "from sklearn.utils import all_estimators\n",
    "\n",
    "from importlib import import_module\n",
    "\n",
    "def feature_selection(df,\n",
    "                       k: int = 5,\n",
    "                       min_votes: float = 0.5,\n",
    "                       label_column: str = None,\n",
    "                       stat_filters: list = ['f_classif', 'f_regression'],\n",
    "                       model_filters: dict = {'LinearRegression': 'LinearRegression',\n",
    "                                             'GradientBoostingRegressor': 'GradientBoostingRegressor'},\n",
    "                       max_scaled_scores: bool = True,\n",
    "                       sample_ratio: float = None):\n",
    "    \n",
    "    \"\"\"Applies selected feature selection statistical functions\n",
    "    or models on our 'df_artifact'.\n",
    "    Each statistical function or model will vote for it's best K selected features.\n",
    "    If a feature has >= 'min_votes' votes, it will be selected.\n",
    "    :param k:                 number of top features to select from each statistical\n",
    "                              function or model.\n",
    "                              \n",
    "    :param min_votes:         minimal number of votes (from a model or by statistical\n",
    "                              function) needed for a feature to be selected.\n",
    "                              Can be specified by percentage of votes or absolute\n",
    "                              number of votes.\n",
    "                              \n",
    "    :param label_column:      ground-truth (y) labels.\n",
    "    \n",
    "    :param stat_filters:      statistical functions to apply to the features\n",
    "                              (from sklearn.feature_selection).\n",
    "                              \n",
    "    :param model_filters:     models to use for feature evaluation, can be specified by\n",
    "                              model name (ex. LinearSVC), formalized json (contains 'CLASS',\n",
    "                              'FIT', 'META') or a path to such json file.\n",
    "                              \n",
    "    :param max_scaled_scores: produce feature scores table scaled with max_scaler.\n",
    "    \n",
    "    :param sample_ratio: percentage of the dataset the user whishes to compute the feature selection process on.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure k is not bigger than the the total number of features\n",
    "    if k > df.shape[1]:\n",
    "        raise ValueError(\n",
    "            f'K cannot be bigger than the total number of features ({df.shape[1]}). Please choose a smaller K.')\n",
    "    elif k < 1:\n",
    "        raise ValueError(f'K cannot be smaller than 1. Please choose a bigger K.')\n",
    "        \n",
    "    # Create a sample dataframe of the original feature vector\n",
    "    if sample_ratio:\n",
    "        df = df.groupby(label_column).apply(lambda x: x.sample(frac=sample_ratio)).reset_index(drop=True)\n",
    "        df = df.dropna()\n",
    "        \n",
    "    # Set feature vector and labels\n",
    "    y = df.pop(label_column)\n",
    "    X = df\n",
    "    \n",
    "    if np.object in list(X.dtypes) and ignore_type_errors is False:\n",
    "        raise ValueError(f\"{df.select_dtypes(include=['object']).columns.tolist()} are neither float or int.\")\n",
    "        \n",
    "    # Create selected statistical estimators\n",
    "    stat_functions_list = {\n",
    "        stat_name: SelectKBest(score_func=create_class(f'sklearn.feature_selection.{stat_name}'), k=k)\n",
    "        for stat_name in stat_filters}\n",
    "    requires_abs = ['chi2']\n",
    "    \n",
    "    # Run statistic filters\n",
    "    selected_features_agg = {}\n",
    "    stats_df = pd.DataFrame(index=X.columns).dropna()\n",
    "    \n",
    "    for stat_name, stat_func in stat_functions_list.items():\n",
    "        try:\n",
    "            params = (X, y) if stat_name in requires_abs else (abs(X), y)\n",
    "            stat = stat_func.fit(*params)\n",
    "\n",
    "            # Collect stat function results\n",
    "            stat_df = pd.DataFrame(index=X.columns,\n",
    "                                   columns=[stat_name],\n",
    "                                   data=stat.scores_)\n",
    "            stats_df = stats_df.join(stat_df)\n",
    "\n",
    "            # Select K Best features\n",
    "            selected_features = X.columns[stat_func.get_support()]\n",
    "            selected_features_agg[stat_name] = selected_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Couldn't calculate {stat_name} because of: {e}\")\n",
    "            \n",
    "    # Create models from class name / json file / json params\n",
    "    all_sklearn_estimators = dict(all_estimators()) if len(model_filters) > 0 else {}\n",
    "    selected_models = {}\n",
    "    for model_name, model in model_filters.items():\n",
    "        if '.json' in model:\n",
    "            current_model = json.load(open(model, 'r'))\n",
    "            ClassifierClass = create_class(current_model[\"META\"][\"class\"])\n",
    "            selected_models[model_name] = ClassifierClass(**current_model[\"CLASS\"])\n",
    "        elif model in all_sklearn_estimators:\n",
    "            selected_models[model_name] = all_sklearn_estimators[model_name]()\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                current_model = json.loads(model) if isinstance(model, str) else current_model\n",
    "                ClassifierClass = create_class(current_model[\"META\"][\"class\"])\n",
    "                selected_models[model_name] = ClassifierClass(**current_model[\"CLASS\"])\n",
    "            except:\n",
    "                context.logger.info(f'unable to load {model}')\n",
    "                \n",
    "    # Run model filters\n",
    "    models_df = pd.DataFrame(index=X.columns)\n",
    "    for model_name, model in selected_models.items():\n",
    "\n",
    "        if model_name == 'LogisticRegression':\n",
    "            model.set_params(solver='liblinear')\n",
    "\n",
    "        # Train model and get feature importance\n",
    "        select_from_model = SelectFromModel(model).fit(X, y)\n",
    "        feature_idx = select_from_model.get_support()\n",
    "        feature_names = X.columns[feature_idx]\n",
    "        selected_features_agg[model_name] = feature_names.tolist()\n",
    "\n",
    "        # Collect model feature importance\n",
    "        if hasattr(select_from_model.estimator_, 'coef_'):\n",
    "            stat_df = select_from_model.estimator_.coef_\n",
    "        elif hasattr(select_from_model.estimator_, 'feature_importances_'):\n",
    "            stat_df = select_from_model.estimator_.feature_importances_\n",
    "\n",
    "        stat_df = pd.DataFrame(index=X.columns,\n",
    "                               columns=[model_name],\n",
    "                               data=stat_df[0])\n",
    "        models_df = models_df.join(stat_df)\n",
    "            \n",
    "    # Create feature_scores DF with stat & model filters scores\n",
    "    result_matrix_df = pd.concat([stats_df, models_df], axis=1, sort=False)\n",
    "\n",
    "    if max_scaled_scores:\n",
    "        normalized_df = result_matrix_df.replace([np.inf, -np.inf], np.nan).values\n",
    "        min_max_scaler = MinMaxScaler()\n",
    "        normalized_df = min_max_scaler.fit_transform(normalized_df)\n",
    "        normalized_df = pd.DataFrame(data=normalized_df,\n",
    "                                     columns=result_matrix_df.columns,\n",
    "                                     index=result_matrix_df.index)\n",
    "\n",
    "    # Create feature count DataFrame\n",
    "    for test_name in selected_features_agg:\n",
    "        result_matrix_df[test_name] = [1 if x in selected_features_agg[test_name] else 0 for x in X.columns]\n",
    "    result_matrix_df.loc[:, 'num_votes'] = result_matrix_df.sum(axis=1)\n",
    "\n",
    "    # How many votes are needed for a feature to be selected?\n",
    "    if isinstance(min_votes, int):\n",
    "        votes_needed = min_votes\n",
    "    else:\n",
    "        num_filters = len(stat_filters) + len(model_filters)\n",
    "        votes_needed = int(np.floor(num_filters * max(min(min_votes, 1), 0)))\n",
    "    print(f'votes needed to be selected: {votes_needed}')\n",
    "\n",
    "    # Create final feature dataframe\n",
    "    selected_features = result_matrix_df[result_matrix_df.num_votes >= votes_needed].index.tolist()\n",
    "    good_feature_df = df.loc[:, selected_features]\n",
    "    final_df = pd.concat([good_feature_df, y], axis=1)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "def create_class(pkg_class: str):\n",
    "    \"\"\"Create a class from a package.module.class string\n",
    "    :param pkg_class:  full class location,\n",
    "                       e.g. \"sklearn.model_selection.GroupKFold\"\n",
    "    \"\"\"\n",
    "    splits = pkg_class.split(\".\")\n",
    "    clfclass = splits[-1]\n",
    "    pkg_module = splits[:-1]\n",
    "    class_ = getattr(import_module(\".\".join(pkg_module)), clfclass)\n",
    "    return class_\n",
    "\n",
    "def OneHotEncoder(x):\n",
    "    l = x.unique().tolist()\n",
    "    encoded = []\n",
    "    for label in x:\n",
    "        encoded.append(l.index(label))\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using generic feature selection as preprocess on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>dataset shape</th>\n",
       "      <th>R^2</th>\n",
       "      <th>Adjusted R^2</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>housing</td>\n",
       "      <td>(506, 12)</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.013903</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.020010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>freMTPL2freq</td>\n",
       "      <td>(678013, 12)</td>\n",
       "      <td>0.315809</td>\n",
       "      <td>0.315795</td>\n",
       "      <td>0.070972</td>\n",
       "      <td>0.0390</td>\n",
       "      <td>0.197484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dataset dataset shape       R^2  Adjusted R^2       MAE     MSE  \\\n",
       "0       housing     (506, 12)  0.999995      0.999995  0.013903  0.0004   \n",
       "1  freMTPL2freq  (678013, 12)  0.315809      0.315795  0.070972  0.0390   \n",
       "\n",
       "       RMSE  \n",
       "0  0.020010  \n",
       "1  0.197484  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = ['housing','freMTPL2freq']\n",
    "baseline_model_scores = []\n",
    "for dataset in datasets:\n",
    "    if( dataset == 'housing' ):\n",
    "        columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'MEDV']\n",
    "        df = pd.read_csv(os.path.join(os.getcwd(), 'datasets', dataset + '.csv'), \n",
    "                         names = columns,\n",
    "                         delimiter = r\"\\s+\")\n",
    "        target = 'MEDV'\n",
    "        \n",
    "    else:\n",
    "        df = pd.read_csv(os.path.join(os.getcwd(), 'datasets', dataset + '.csv'))\n",
    "        target = 'ClaimNb'\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            df[col] = OneHotEncoder(df[col])\n",
    "                    \n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "        \n",
    "    xgbr= XGBRegressor()\n",
    "#     Training the model\n",
    "    xgbr.fit(X_train, y_train)\n",
    "    \n",
    "#     predicting the model\n",
    "    y_pred=xgbr.predict(X_train)\n",
    "    \n",
    "#     Model Evaluation and error calculations\n",
    "    baseline_model_scores.append({\n",
    "            'dataset': dataset,\n",
    "            'dataset shape': df.shape,\n",
    "            'R^2': metrics.r2_score(y_train, y_pred),\n",
    "            'Adjusted R^2': 1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1),\n",
    "            'MAE': metrics.mean_absolute_error(y_train, y_pred),\n",
    "            'MSE': metrics.mean_squared_error(y_train, y_pred),\n",
    "            'RMSE': np.sqrt(metrics.mean_squared_error(y_train, y_pred))\n",
    "    })\n",
    "    \n",
    "df = pd.DataFrame(data = baseline_model_scores)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "votes needed to be selected: 2\n",
      "votes needed to be selected: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>dataset shape</th>\n",
       "      <th>R^2</th>\n",
       "      <th>Adjusted R^2</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>housing</td>\n",
       "      <td>(486, 10)</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.020620</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.030084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>freMTPL2freq</td>\n",
       "      <td>(610213, 9)</td>\n",
       "      <td>0.318830</td>\n",
       "      <td>0.318819</td>\n",
       "      <td>0.071106</td>\n",
       "      <td>0.039751</td>\n",
       "      <td>0.199377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dataset dataset shape       R^2  Adjusted R^2       MAE       MSE  \\\n",
       "0       housing     (486, 10)  0.999989      0.999989  0.020620  0.000905   \n",
       "1  freMTPL2freq   (610213, 9)  0.318830      0.318819  0.071106  0.039751   \n",
       "\n",
       "       RMSE  \n",
       "0  0.030084  \n",
       "1  0.199377  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_selection_model_scores = []\n",
    "for dataset in datasets:\n",
    "    if( dataset == 'housing' ):\n",
    "        columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'MEDV']\n",
    "        df = pd.read_csv(os.path.join(os.getcwd(), 'datasets', dataset + '.csv'), \n",
    "                         names = columns,\n",
    "                         delimiter = r\"\\s+\")\n",
    "        target = 'MEDV'\n",
    "        \n",
    "    else:\n",
    "        df = pd.read_csv(os.path.join(os.getcwd(), 'datasets', dataset + '.csv'))\n",
    "        target = 'ClaimNb'\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            df[col] = OneHotEncoder(df[col])\n",
    "            \n",
    "    df = feature_selection(df,k=int(0.8*len(df.columns)),label_column=target, sample_ratio=0.9)   \n",
    "    X = df.drop([target],axis=1, inplace=False)\n",
    "    y = df[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "        \n",
    "    xgbr= XGBRegressor()\n",
    "#     Training the model\n",
    "    xgbr.fit(X_train, y_train)\n",
    "    \n",
    "#     predicting the model\n",
    "    y_pred=xgbr.predict(X_train)\n",
    "    \n",
    "#     Model Evaluation and error calculations\n",
    "    feature_selection_model_scores.append({\n",
    "            'dataset': dataset,\n",
    "            'dataset shape': df.shape,\n",
    "            'R^2': metrics.r2_score(y_train, y_pred),\n",
    "            'Adjusted R^2': 1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1),\n",
    "            'MAE': metrics.mean_absolute_error(y_train, y_pred),\n",
    "            'MSE': metrics.mean_squared_error(y_train, y_pred),\n",
    "            'RMSE': np.sqrt(metrics.mean_squared_error(y_train, y_pred))\n",
    "    })\n",
    "    \n",
    "df2 = pd.DataFrame(data = feature_selection_model_scores)\n",
    "df2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
